{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'storage'\n",
    "label_csv = f'storage/train.csv'\n",
    "n = len(list(open(label_csv)))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(str, pd.read_csv(label_csv)['Target']))\n",
    "labels = [[int(ll) for ll in l.split()] for l in labels]\n",
    "ohlabels = np.zeros((n, 28))\n",
    "for i, labs in enumerate(labels):\n",
    "    for c in labs:        \n",
    "        ohlabels[i, int(c)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_NUM = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/trent-b/iterative-stratification\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "stratified = list(msss.split(np.arange(n), ohlabels))\n",
    "trn_idxs = np.array(list(stratified[FOLD_NUM][0]))\n",
    "val_idxs = np.array(list(stratified[FOLD_NUM][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88452, 22019, 110471, 110471)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stratified[0][0]), len(stratified[0][1]), len(labels), len(stratified[0][0])+len(stratified[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn label props {0: 0.37771898883009997, 1: 0.0277890833446389, 2: 0.10290326956993624, 3: 0.030988558766336546, 4: 0.04681635237190793, 5: 0.05566861122416678, 6: 0.03504725726947949, 7: 0.08594491927825261, 8: 0.009813232035454257, 9: 0.008908786686564465, 10: 0.00823045267489712, 11: 0.02020304798082576, 12: 0.021231854565187898, 13: 0.013182290960068737, 14: 0.025324469768914212, 15: 0.011396011396011397, 16: 0.015612987835210057, 17: 0.008072174738841406, 18: 0.01824718491385158, 19: 0.033928006150228375, 20: 0.007925202369646814, 21: 0.12874779541446207, 22: 0.025754081309636866, 23: 0.09443539999095554, 24: 0.0077443132998688555, 25: 0.3471826527382083, 26: 0.006387645276534165, 27: 0.005743227965450188}\n",
      "val label props {0: 0.3793541941050911, 1: 0.027930423724964803, 2: 0.10336527544393478, 3: 0.031109496344066487, 4: 0.047004859439574914, 5: 0.05590626277305963, 6: 0.03519687542576865, 7: 0.08628911394704573, 8: 0.009855125119215224, 9: 0.008946818656614742, 10: 0.008265588809664381, 11: 0.02030064943912076, 12: 0.02129978654798129, 13: 0.01326127435396703, 14: 0.02543258095281348, 15: 0.011444661428766066, 16: 0.015668286479858306, 17: 0.008083927517144285, 18: 0.018347790544529723, 19: 0.034061492347518055, 20: 0.007947681547754212, 21: 0.12929742495117852, 22: 0.025841318860983696, 23: 0.09482719469549025, 24: 0.007766020255234116, 25: 0.34865343566919477, 26: 0.006403560561333394, 27: 0.005767746037513057}\n"
     ]
    }
   ],
   "source": [
    "labels_num_flat = [item for sublist in np.array(labels)[trn_idxs] for item in sublist]\n",
    "label_counts = dict((x,labels_num_flat.count(x)/len(trn_idxs)) for x in range(28)) # \n",
    "print('trn label props',label_counts)\n",
    "labels_num_flat = [item for sublist in np.array(labels)[val_idxs] for item in sublist]\n",
    "label_counts = dict((x,labels_num_flat.count(x)/len(val_idxs)) for x in set(labels_num_flat)) # \n",
    "print('val label props',label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = resnext50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvnetBuilder_custom():\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 0,0\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(pretrained), cut)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with 3rd weights\n",
    "        w = layers[0].weight.data\n",
    "        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        #layers[0].weight = torch.nn.Parameter(torch.cat((w,w[2]),dim=1)) # my attempt...\n",
    "        layers[0].weight = torch.nn.Parameter(torch.cat((w,w[:,:1,:,:]),dim=1)) # copied from kaggle post        \n",
    "        #layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))  \n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "    @property\n",
    "    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "    def create_fc_layer(self, ni, nf, p, actn=None):\n",
    "        res=[nn.BatchNorm1d(num_features=ni)]\n",
    "        if p: res.append(nn.Dropout(p=p))\n",
    "        res.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "        if actn: res.append(actn)\n",
    "        return res\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        res=[]\n",
    "        ni=self.nf\n",
    "        for i,nf in enumerate(self.xtra_fc):\n",
    "            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "            ni=nf\n",
    "        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n",
    "        if self.is_reg: final_actn = None\n",
    "        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "        return res\n",
    "\n",
    "    def get_layer_groups(self, do_fc=False):\n",
    "        if do_fc:\n",
    "            return [self.fc_model]\n",
    "        idxs = [self.lr_cut]\n",
    "        c = children(self.top_model)\n",
    "        if len(c)==3: c = children(c[0])+c[1:]\n",
    "        lgs = list(split_by_idxs(c,idxs))\n",
    "        return lgs+[self.fc_model]\n",
    "    \n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, models, precompute=False, **kwargs):\n",
    "        self.precompute = False\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n",
    "            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n",
    "        if precompute: self.save_fc1()\n",
    "        self.freeze()\n",
    "        self.precompute = precompute\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n",
    "\n",
    "        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss    \n",
    "    \n",
    "    @classmethod\n",
    "    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "        return cls(data, models, precompute, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n",
    "        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n",
    "        convlearn=cls(data, models, precompute, **kwargs)\n",
    "        convlearn.lsuv_init()\n",
    "        return convlearn\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.models.fc_model if self.precompute else self.models.model\n",
    "    \n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n",
    "        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.fc_data if self.precompute else self.data_\n",
    "\n",
    "    def create_empty_bcolz(self, n, name):\n",
    "        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n",
    "\n",
    "    def set_data(self, data, precompute=False):\n",
    "        super().set_data(data)\n",
    "        if precompute:\n",
    "            self.unfreeze()\n",
    "            self.save_fc1()\n",
    "            self.freeze()\n",
    "            self.precompute = True\n",
    "        else:\n",
    "            self.freeze()\n",
    "\n",
    "    def get_layer_groups(self):\n",
    "        return self.models.get_layer_groups(self.precompute)\n",
    "\n",
    "    def summary(self):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        res = super().summary()\n",
    "        self.precompute = precompute\n",
    "        return res\n",
    "\n",
    "    def get_activations(self, force=False):\n",
    "        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n",
    "        # TODO: Somehow check that directory names haven't changed (e.g. added test set)\n",
    "        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n",
    "        if os.path.exists(names[0]) and not force:\n",
    "            self.activations = [bcolz.open(p) for p in names]\n",
    "        else:\n",
    "            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n",
    "\n",
    "    def save_fc1(self):\n",
    "        self.get_activations()\n",
    "        act, val_act, test_act = self.activations\n",
    "        m=self.models.top_model\n",
    "        if len(self.activations[0])!=len(self.data.trn_ds):\n",
    "            predict_to_bcolz(m, self.data.fix_dl, act)\n",
    "        if len(self.activations[1])!=len(self.data.val_ds):\n",
    "            predict_to_bcolz(m, self.data.val_dl, val_act)\n",
    "        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n",
    "            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n",
    "\n",
    "        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n",
    "                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n",
    "                test = test_act if self.data.test_dl else None, num_workers=8)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(0)\n",
    "        self.precompute = False\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        pred = super().predict_array(arr)\n",
    "        self.precompute = precompute\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def open_multichannel_image(path, id, colours):    \n",
    "    flags = cv2.IMREAD_GRAYSCALE    \n",
    "    img = [cv2.imread(os.path.join(path, id[:-4] + '_' + colour + '.png'), flags) for colour in colours]    \n",
    "    sz = 512    \n",
    "    try:\n",
    "        img = [im.astype(np.float32)/255 for im in img]\n",
    "    except:\n",
    "        print('hi1', path+id)\n",
    "    try:\n",
    "        stacked = np.stack(img, axis=-1)\n",
    "    except:\n",
    "        print('hi2', path+id)\n",
    "        print(img)\n",
    "    return stacked\n",
    "\n",
    "channel_names = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "def get_x_multichannel(self, i):\n",
    "    #print(self.fnames)\n",
    "    #a\n",
    "    img = open_multichannel_image(self.path, self.fnames[i], channel_names)\n",
    "    if self.sz == 512: return img # change to match your image dimensions\n",
    "    else: return cv2.resize(img, (self.sz, self.sz), cv2.INTER_AREA) \n",
    "\n",
    "# monkey patching, must be done before new class is defined and had to be this specific class\n",
    "FilesNhotArrayDataset.get_x = get_x_multichannel\n",
    "    \n",
    "class MultichannelImageClassifierData(ImageData):\n",
    "    def set_weighted_dls(self):                \n",
    "        self.trn_dl = DataLoader(self.trn_ds, batch_size=self.bs, shuffle=False, num_workers=self.num_workers, pin_memory=False, sampler=trn_weighted_sampler)\n",
    "        #self.val_dl = DataLoader(self.val_ds, batch_size=self.bs, shuffle=False, num_workers=self.num_workers, pin_memory=False, sampler=val_weighted_sampler)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, path, folder, csv_fname, channel_names=channel_names, bs=64, tfms=(None,None),\n",
    "               val_idxs=None, suffix='', channel_sep='_', test_name=None, continuous=False, skip_header=True, num_workers=8, cat_separator=' '):\n",
    "        assert not (tfms[0] is None or tfms[1] is None), \"please provide transformations for your train and validation sets\"\n",
    "        assert not (os.path.isabs(folder)), \"folder needs to be a relative path\"\n",
    "        fnames,y,classes = csv_source(folder, csv_fname, skip_header, suffix, continuous=continuous)                      \n",
    "        \n",
    "        # from_names_and_array\n",
    "        val_idxs = get_cv_idxs(len(fnames)) if val_idxs is None else val_idxs\n",
    "        ((val_fnames,trn_fnames),(val_y,trn_y)) = split_by_idx(val_idxs, np.array(fnames), y)        \n",
    "            \n",
    "        test_ids = pd.read_csv('storage/sample_submission.csv', index_col=0, header=0)        \n",
    "        \n",
    "        test_fnames = []\n",
    "        for i in range(len(test_ids)):\n",
    "            test_fnames.append(test_name + '/' + test_ids.index[i] + suffix)                                            \n",
    "            \n",
    "        f = FilesNhotArrayDataset # multiple classes per label            \n",
    "        datasets = cls.get_ds(f, (trn_fnames,trn_y), (val_fnames,val_y), tfms,\n",
    "                               path=path, test=test_fnames)\n",
    "        return cls(path, datasets, bs, num_workers, classes=classes)\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    #mean and std in of each channel in the train set\n",
    "    stats = A([0.06734, 0.05087, 0.03266, 0.09257],[0.11997, 0.10335, 0.10124, 0.1574])\n",
    "    #stats = A([0.08069, 0.05258, 0.05487] #, 0.08282]\n",
    "    #          , [0.13704, 0.10145, 0.15313]) #, 0.13814])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    md = MultichannelImageClassifierData.from_csv(PATH, 'train', label_csv, bs=bs, tfms=tfms,\n",
    "                    suffix='.png', val_idxs=val_idxs, test_name='test')      \n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=64\n",
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "def f1_macro(preds, targs, start=0.0, end=0.99, step=0.01):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return max([fbeta_score(targs, (preds>th), 1, average='macro')\n",
    "                    for th in np.arange(start,end,step)])\n",
    "learn = ConvLearner.pretrained(f_model, data, metrics=[f1_macro])\n",
    "learn.opt_fn = optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "        (1): Lambda(\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce(\n",
       "      )\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mp): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (9): Flatten(\n",
       "  )\n",
       "  (10): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (11): Dropout(p=0.25)\n",
       "  (12): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (15): Dropout(p=0.5)\n",
       "  (16): Linear(in_features=512, out_features=28, bias=True)\n",
       "  (17): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d85d59700c49f6af6f30a50aae8a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1014/1383 [07:57<02:53,  2.13it/s, loss=0.675]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf=learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00075\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab19e0e9e4a841c69f4f4ae1c2d4723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.162111   0.154103   0.11718   \n",
      "    1      0.156058   0.152184   0.123266  \n",
      "    2      0.152983   0.1512     0.125992  \n",
      "    3      0.152185   0.15142    0.125404  \n",
      "    4      0.153138   0.150391   0.129162  \n",
      "    5      0.152564   0.149585   0.130112  \n",
      "    6      0.151545   0.149317   0.131042  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.14932]), 0.13104163987944192]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a7b3c975ef4b019348074ee82626f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.140558   0.138118   0.156588  \n",
      "    1      0.136513   0.132562   0.177857  \n",
      "    2      0.131745   0.127712   0.192168  \n",
      "    3      0.13013    0.128037   0.194062  \n",
      "    4      0.126298   0.123446   0.210164  \n",
      "    5      0.120442   0.119569   0.221651  \n",
      "    6      0.117493   0.118305   0.224726  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fec470e6124b189e3dda4f409ac1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.123878   0.119901   0.23438   \n",
      "    1      0.119553   0.118429   0.240039  \n",
      "    2      0.117273   0.117831   0.239714  \n",
      "    3      0.119003   0.117452   0.240255  \n",
      "    4      0.120319   0.116941   0.241498  \n",
      "    5      0.118903   0.116674   0.24341   \n",
      "    6      0.118445   0.116108   0.245204  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11611]), 0.24520449593146432]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(sz, bs)\n",
    "learn.set_data(data)\n",
    "learn.freeze()\n",
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f847c43b7424b8590e9e247d47b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.112364   0.104042   0.282936  \n",
      "    1      0.105913   0.103919   0.283544  \n",
      "    2      0.09932    0.097532   0.304292  \n",
      "    3      0.104012   0.103449   0.292556  \n",
      "    4      0.095868   0.096814   0.309969  \n",
      "    5      0.092647   0.091957   0.325162  \n",
      "    6      0.089339   0.090769   0.327495  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a4eaef989f45fcab740a635ca614d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.100935   0.097703   0.312515  \n",
      "    1      0.096504   0.095585   0.317714  \n",
      "    2      0.095525   0.094753   0.317897  \n",
      "    3      0.095478   0.094033   0.321911  \n",
      "    4      0.095079   0.09318    0.322996  \n",
      "    5      0.093125   0.09267    0.326737  \n",
      "    6      0.091165   0.092689   0.326104  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_data(sz, bs)\n",
    "learn.set_data(data)\n",
    "learn.freeze()\n",
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691752a6c41d41518a4a8ec5de6c3e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.082292   0.08124    0.361232  \n",
      "    1      0.083728   0.081818   0.362819  \n",
      "    2      0.07508    0.07753    0.375586  \n",
      "    3      0.081399   0.082433   0.36049   \n",
      "    4      0.076355   0.078184   0.374366  \n",
      "    5      0.069544   0.074726   0.388429  \n",
      "    6      0.062191   0.074713   0.386072  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00075\n",
    "lrs = np.array([lr/6,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a1381961c40b09c531c8fd401d3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.093709   0.095382   0.328231  \n",
      "    1      0.091559   0.091797   0.335949  \n",
      "    2      0.092162   0.091331   0.336966  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_data(sz, bs)\n",
    "learn.set_data(data)\n",
    "learn.freeze()\n",
    "learn.fit(lr, 2, cycle_len=1, cycle_mult=2)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4828f0b3ab6749cebe720a718b3c2e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.088621   0.09014    0.33857   \n",
      "    1      0.087534   0.089007   0.342843  \n",
      "    2      0.089799   0.088767   0.342419  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=3, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_frozen_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "data = get_data(sz, bs)\n",
    "learn.set_data(data)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5c09d19d6d4c4d88789dece0c1b79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.0851     0.077413   0.233093  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_1') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcdde55c5b74f489f8665a155414e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.080031   0.075666   0.236007  \n",
      "    1      0.071234   0.07361    0.237766  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_2') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aabf1d3b98943209bff632bed34d1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.074512   0.074232   0.237338  \n",
      "  4%|▍         | 238/5529 [04:43<1:45:03,  1.19s/it, loss=0.0705]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 3789/5529 [1:15:32<34:41,  1.20s/it, loss=0.0709]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      0.073997   0.074072   0.236251  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_3') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457c145b94754cd58cd7ea7984874e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.070225   0.073886   0.237649  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_4') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1d930bd4c04af48c7ccae5ff77f014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.075955   0.074231   0.237137  \n",
      "    1      0.073631   0.075119   0.239325  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_5') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff737f65acd4e1ab78d15f788c93136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.074519   0.073989   0.238157  \n",
      "    1      0.069061   0.074219   0.237353  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_6') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd790a29c9a048918e28c368fd70cdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.070332   0.073873   0.237586  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b733e73e35341028497f54367b06af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.075232   0.073596   0.238024  \n",
      "    1      0.076669   0.074161   0.237037  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.000000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f042a167474009b424cf7f3f3ff3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.072917   0.074252   0.239079  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d6399d94e4b1097be1863a755f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.069617   0.074205   0.238756  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000000001\n",
    "lrs = np.array([lr/9,lr/3,lr])\n",
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823188f2f9224a728a014c9cc18c94e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.071424   0.074188   0.238992  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00000000005\n",
    "lrs = np.array([lr/12,lr/6,lr])\n",
    "learn.fit(lrs, 1, cycle_len=1, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421ec82061f04b6381d9cb0cf60374ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1_macro   \n",
      "    0      0.072009   0.073837   0.238648  \n",
      "    1      0.069437   0.074145   0.238377  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000000001\n",
    "lrs = np.array([lr/12,lr/6,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tta_test = learn.TTA(is_test=True)\n",
    "prob_preds = tta_test[0]\n",
    "classes = np.array(data.classes, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [' '.join(classes[np.where(pred > .2)]) for pred in prob_preds[0]]\n",
    "with open('protein.rgby_external_resnext_stratified_4_.2t.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score .532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [' '.join(classes[np.where(pred > .3)]) for pred in prob_preds[0]]\n",
    "with open('protein.rgby_external_resnext_stratified_4_.3t.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score .532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [' '.join(classes[np.where(pred > .3)]) for pred in prob_preds[0]]\n",
    "with open('protein.rgby_external_resnext_stratified_1_.3t.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score .531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [' '.join(classes[np.where(pred > .5)]) for pred in prob_preds[0]]\n",
    "with open('protein.rgby_external_resnext_stratified_1_.5t.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/69984#432870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_y = model.predict(valid_x)\n",
    "preds_y = val_preds\n",
    "valid_y = val_targs\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# one threshold for all\n",
    "thresholds = np.linspace(0, 1, 1500)\n",
    "score = 0.0\n",
    "best_threshold=0.0\n",
    "best_val = 0.0\n",
    "for threshold in thresholds:\n",
    "    score = f1_score(valid_y > 0.5, preds_y > threshold, average='macro')\n",
    "    if score > best_val:\n",
    "        best_threshold = threshold\n",
    "        best_val = score\n",
    "    print(\"Threshold %0.4f, F1: %0.4f\" % (threshold,score))\n",
    "\n",
    "print(\"BEST: %0.5f, F1: %0.5f\" % (best_threshold,best_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = learn.predict()\n",
    "val_targs = data.val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[0] 0.371371, F1: 0.708299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[1] 0.305305, F1: 0.708351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[2] 0.644645, F1: 0.709063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[3] 0.310310, F1: 0.710006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[4] 0.342342, F1: 0.710767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[5] 0.250250, F1: 0.711814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[6] 0.311311, F1: 0.712269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[7] 0.813814, F1: 0.712893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[8] 0.166166, F1: 0.713633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[9] 0.384384, F1: 0.714126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[10] 0.369369, F1: 0.716238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[11] 0.310310, F1: 0.716767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[12] 0.416416, F1: 0.716949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[13] 0.247247, F1: 0.718884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[14] 0.255255, F1: 0.720334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[15] 0.142142, F1: 0.722447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[16] 0.311311, F1: 0.722628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[17] 0.557558, F1: 0.722829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[18] 0.343343, F1: 0.723487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[19] 0.381381, F1: 0.725710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[20] 0.816817, F1: 0.728128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[21] 0.252252, F1: 0.729268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[22] 0.385385, F1: 0.729606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[23] 0.357357, F1: 0.730155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[24] 0.363363, F1: 0.730834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[25] 0.352352, F1: 0.731368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold[26] 0.922923, F1: 0.731964\n",
      "Threshold[27] 0.604605, F1: 0.732623\n",
      "Best threshold: \n",
      "[0.37137 0.30531 0.64464 0.31031 0.34234 0.25025 0.31131 0.81381 0.16617 0.38438 0.36937 0.31031 0.41642\n",
      " 0.24725 0.25526 0.14214 0.31131 0.55756 0.34334 0.38138 0.81682 0.25225 0.38539 0.35736 0.36336 0.35235\n",
      " 0.92292 0.6046 ]\n",
      "Best f1:\n",
      "[0.7083  0.70835 0.70906 0.71001 0.71077 0.71181 0.71227 0.71289 0.71363 0.71413 0.71624 0.71677 0.71695\n",
      " 0.71888 0.72033 0.72245 0.72263 0.72283 0.72349 0.72571 0.72813 0.72927 0.72961 0.73015 0.73083 0.73137\n",
      " 0.73196 0.73262]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#preds_y = model.predict(valid_x)\n",
    "preds_y = val_preds\n",
    "valid_y = val_targs\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# threshold for each class\n",
    "thresholds = np.linspace(0, 1, 1000)\n",
    "score = 0.0\n",
    "test_threshold=0.5*np.ones(28)\n",
    "best_threshold=np.zeros(28)\n",
    "best_val = np.zeros(28)\n",
    "for i in range(28):\n",
    "    for threshold in thresholds:\n",
    "        test_threshold[i] = threshold\n",
    "        max_val = np.max(preds_y)\n",
    "        val_predict = (preds_y > test_threshold)\n",
    "        #score = f1_score(valid_y > 0.5, val_predict, average='macro')\n",
    "        score = f1_score(valid_y, val_predict, average='macro')\n",
    "        if score > best_val[i]:\n",
    "            best_threshold[i] = threshold\n",
    "            best_val[i] = score\n",
    "\n",
    "    print(\"Threshold[%d] %0.6f, F1: %0.6f\" % (i,best_threshold[i],best_val[i]))\n",
    "    test_threshold[i] = best_threshold[i]\n",
    "\n",
    "print(\"Best threshold: \")\n",
    "print(best_threshold)\n",
    "print(\"Best f1:\")\n",
    "print(best_val)\n",
    "\n",
    "val_thresholds4 = best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_thresholds_mean = np.mean([val_thresholds4,val_thresholds5,val_thresholds6], axis=0)\n",
    "val_thresholds_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(data.classes, dtype=str)\n",
    "res = [' '.join(classes[np.where(pred > val_thresholds4)]) for pred in prob_preds[0]]\n",
    "with open(f'protein.rgby_external_resnext_stratified_fold{FOLD_NUM}_val_thresholds.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score .531\n",
    "            \n",
    "res = [' '.join(classes[np.where(pred > val_thresholds4*0.5)]) for pred in prob_preds[0]]\n",
    "with open(f'protein.rgby_external_resnext_stratified_fold{FOLD_NUM}_val_thresholdsx0.5.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Predicted\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(data.test_ds.fnames[i][5:-4] + ',' + res[i] + '\\n')  # score .531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'protein.preds_fold{FOLD_NUM}.csv', 'w') as res_file:\n",
    "        res_file.write('Id,Preds\\n')\n",
    "        for i in range(len(data.test_ds.fnames)):\n",
    "            res_file.write(' '.join(map(str, prob_preds[0][i])) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'protein.threshs_fold{FOLD_NUM}.csv', 'w') as res_file:\n",
    "        res_file.write('Thresholds\\n')\n",
    "        res_file.write(' '.join(map(str, val_thresholds4)) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ba9660447a45999d4b685214d258db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1         \n",
      "    0      0.069824   0.073369   0.257502  \n",
      "    1      0.072701   0.072908   0.258255  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lr = 0.000005\n",
    "lr = 0.000000005\n",
    "lrs = np.array([lr/12,lr/6,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_5') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_5') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdcef9a4baf4bc1925bc221e7529be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   f1         \n",
      "    0      0.07531    0.072817   0.25792   \n",
      "    1      0.074533   0.073054   0.257885  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lr = 0.000005\n",
    "lr = 0.00000001\n",
    "lrs = np.array([lr/12,lr/6,lr])\n",
    "learn.fit(lrs, 1, cycle_len=2, cycle_mult=1)\n",
    "learn.save(f'rgb_external_resnext_stratified_{sz}_fold{FOLD_NUM}_6') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data = pd.read_csv('protein/subcellular_location.tsv', sep='\\t')\n",
    "ext_fnames = [a+'-'+b for a,b in zip(ext_data['Gene'], ext_data['Gene name'])]\n",
    "ext_labels = ext_data.iloc[:,3:10].copy() # probably dont need to copy()?\n",
    "\n",
    "import csv\n",
    "from itertools import chain\n",
    "import os\n",
    "all_ext_files = os.listdir('protein/external_data')\n",
    "ext_data_exp = []\n",
    "for i in range(len(ext_data)):\n",
    "    if ext_data['Reliability'][i] != 'Uncertain':\n",
    "        labs = []        \n",
    "        for lab in ext_labels.iloc[i,:]:\n",
    "            if isinstance(lab, str):\n",
    "                ids = ([[str(k) for k,v in name_label_dict.items() if v == l] for l in lab.split(';')])\n",
    "                ids = [' '.join(list(chain.from_iterable(ids)))]\n",
    "                if ids != ['']:\n",
    "                    labs = labs + ids \n",
    "        #if ext_fnames[i]==\"ENSG00000005381-MPO\":\n",
    "        #        print(labs, labs==[''], labs==[])\n",
    "        #        a\n",
    "        if labs != []:\n",
    "            j = 0   \n",
    "            while True:\n",
    "                jname = ext_fnames[i] + '_' + str(j) + '.png'\n",
    "                if jname not in all_ext_files: break\n",
    "                img = cv2.imread('protein/external_data/' + jname).astype(np.float32)\n",
    "                b,g,r = cv2.split(img)\n",
    "                sz = 512                                \n",
    "                for imag, colour in zip([b,g,r],['blue', 'green', 'red']):   \n",
    "                    p=300                    \n",
    "                    img = np.int16(imag)          \n",
    "                    img = img * 1.275\n",
    "                    img = np.clip(img, 0, 255)\n",
    "                    img = np.uint8(img)       \n",
    "                    img =cv2.copyMakeBorder(img,p,p,p,p,cv2.BORDER_CONSTANT,value=[0])\n",
    "                    img = cv2.resize(img, (sz, sz), cv2.INTER_AREA)                        \n",
    "                    #cv2.imwrite('protein/external_proc/' + fnames[i], img)\n",
    "                    cv2.imwrite('protein/e_rgb/' + jname[:-4] + '_' + colour + '.png', img)\n",
    "                #cv2.imwrite('protein/e_rgb/' + jname[:-4] + '_blue.png', cv2.resize(b, (sz, sz), cv2.INTER_AREA))\n",
    "                #cv2.imwrite('protein/e_rgb/' + jname[:-4] + '_green.png', cv2.resize(g, (sz, sz), cv2.INTER_AREA))\n",
    "                #cv2.imwrite('protein/e_rgb/' + jname[:-4] + '_red.png', cv2.resize(r, (sz, sz), cv2.INTER_AREA))                        \n",
    "                ext_data_exp.append([ext_fnames[i] + '_' + str(j), labs[0]])\n",
    "                j+=1\n",
    "\n",
    "\n",
    "with open('protein/ext_output.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(ext_data_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_counts {0: 40958, 1: 3072, 2: 10871, 3: 3329, 4: 5130, 5: 5938, 6: 3725, 7: 9405, 8: 217, 9: 197, 10: 182, 11: 2194, 12: 2233, 13: 1458, 14: 2692, 15: 63, 16: 1290, 17: 446, 18: 1893, 19: 3672, 20: 438, 21: 13809, 22: 2729, 23: 10345, 24: 428, 25: 37366, 26: 706, 27: 127}\n",
      "max_label_count 40958\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/jschnab/exploring-the-human-protein-atlas-images\n",
    "# not including the high labels:\n",
    "# for each label and each class, duplicate by\n",
    "# max(1 - label_count / max_label_count) * copy_number\n",
    "import csv\n",
    "import pandas as pd\n",
    "train_csv = pd.read_csv('../storage/train.csv')\n",
    "copy_dict = {'8': 4,\n",
    "             '9': 4,\n",
    "             '10': 4,\n",
    "             '15': 19,\n",
    "             '17': 1,\n",
    "             '20': 1,\n",
    "             '24': 1,\n",
    "             '27': 4}\n",
    "labels_num = [value.split() for value in train_csv['Target'].astype(str)]\n",
    "labels_num_flat = list(map(int, [item for sublist in labels_num for item in sublist]))\n",
    "label_counts = dict((x,labels_num_flat.count(x)) for x in set(labels_num_flat))\n",
    "max_label_count = max(label_counts.values())\n",
    "dup_data_exp = []\n",
    "from pathlib import Path\n",
    "print('label_counts',label_counts)\n",
    "print('max_label_count',max_label_count)\n",
    "from shutil import copyfile\n",
    "for id, targs in zip(train_csv['Id'], train_csv['Target']):    \n",
    "    label_increase = [copy_dict.get(c, 0) for c in targs.split()]        \n",
    "    if max(label_increase)>0: \n",
    "        n_dups = max(label_increase)\n",
    "        # copy the file n_dups times\n",
    "        for n in range(n_dups):\n",
    "            d_id = id + 'dup_' + str(n)\n",
    "            copyfile('../storage/train/' + id + '_red.png', '../storage/dups/' + d_id + '_red.png')\n",
    "            copyfile('../storage/train/' + id + '_green.png', '../storage/dups/' + d_id + '_green.png')\n",
    "            copyfile('../storage/train/' + id + '_blue.png', '../storage/dups/' + d_id + '_blue.png')\n",
    "            copyfile('../storage/train/' + id + '_yellow.png', '../storage/dups/' + d_id + '_yellow.png')\n",
    "            dup_data_exp.append([d_id, targs])\n",
    "        \n",
    "with open('../storage/dup_output.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(dup_data_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('../storage/train.csv')\n",
    "for i, v in zip(train_csv['Id'], train_csv['Target']):\n",
    "    if isinstance(v, float):\n",
    "        print('n', i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_counts {0: 41763, 1: 3073, 2: 11378, 3: 3426, 4: 5176, 5: 6155, 6: 3875, 7: 9502, 8: 1085, 9: 985, 10: 910, 11: 2234, 12: 2347, 13: 1458, 14: 2800, 15: 1260, 16: 1726, 17: 892, 18: 2018, 19: 3751, 20: 876, 21: 14235, 22: 2847, 23: 10441, 24: 856, 25: 38386, 26: 706, 27: 635}\n",
      "max_label_count 41763\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_csv = pd.read_csv('../storage/train.csv')\n",
    "copy_number = 1\n",
    "labels_num = [value.split() for value in train_csv['Target'].astype(str)]\n",
    "labels_num_flat = list(map(int, [item for sublist in labels_num for item in sublist]))\n",
    "label_counts = dict((x,labels_num_flat.count(x)) for x in set(labels_num_flat))\n",
    "max_label_count = max(label_counts.values())\n",
    "print('label_counts',label_counts)\n",
    "print('max_label_count',max_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cv2.IMREAD_GRAYSCALE    \n",
    "#import random\n",
    "#r = random.randint(100, 1000)\n",
    "\n",
    "fnames = os.listdir('protein/external_data')\n",
    "for i in range(len(fnames)):        \n",
    "    img = cv2.imread('protein/external_data/' + fnames[i], flags)*1.0    \n",
    "    p=300        \n",
    "    #kernel = np.array([[0,-1,0], [-1,6,-1], [0,-1,0]])\n",
    "    #img = cv2.filter2D(img, -1, kernel)    \n",
    "    #contrast = 15\n",
    "    #brightness = 30\n",
    "    img = np.int16(img)\n",
    "    #img = img * (contrast/127+1) - contrast + brightness\n",
    "    img = img * 1.275\n",
    "    img = np.clip(img, 0, 255)\n",
    "    img = np.uint8(img)       \n",
    "    img =cv2.copyMakeBorder(img,p,p,p,p,cv2.BORDER_CONSTANT,value=[0])\n",
    "    img = cv2.resize(img, (512, 512), cv2.INTER_AREA)    \n",
    "    #cv2.imshow('a',img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()    \n",
    "    cv2.imwrite('protein/external_proc/' + fnames[i], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0,1000)\n",
    "pp = learn.predict_array([data.trn_ds[i][0]])\n",
    "classes = np.array(data.classes, dtype=str)\n",
    "[' '.join(classes[np.where(pred > val_thresholds6*.5)]) for pred in pp], [' '.join(classes[np.where(d==1)]) for d in [data.trn_ds.y[0]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
